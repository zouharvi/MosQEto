#### MODEL SPECIFIC OPTIONS ####
#
model: quetch

output-dir: runs/quetch
wmt18-format: true

window-size: 3
max-aligned: 5

## embeddings
#source-embeddings-size: 50
#source-pos-embeddings-size: 20
#target-embeddings-size: 50
#target-pos-embeddings-size: 20

# network
#hidden-sizes: [100, 100, 300]
# output-size: 50
dropout: 0.1
embeddings-dropout: 0.5
freeze-embeddings: false
bad-weight: 3.0

# initialization
init-support: 0.1
init-type: uniform

#### Pretrained Embedding Options ###
## pip-install the polyglot package to use these
##embeddings-format: polyglot
##source-embeddings: path/to/source/embeddings_pkl.tar.bz2
##target-embeddings: path/to/target/embeddings_pkl.tar.bz2

#
# TRAINING OPTIONS
#
epochs: 50
train-batch-size: 64
valid-batch-size: 64

log-interval: 100
checkpoint-save: true
checkpoint-keep-only-best: 1
checkpoint-early-stop-patience: 10

optimizer: adam
learning-rate: 0.001
#learning-rate-decay: 0.92
#learning-rate-decay-start: 5

#
# DATA OPTIONS
#
train-source: data_all/train/train.src
train-target: data_all/train/train.mt
train-target-tags: data_all/train/train.tags
train-alignments: data_all/train/train.src-mt.alignments

valid-source: data_all/dev/dev.src
valid-target: data_all/dev/dev.mt
valid-target-tags: data_all/dev/dev.tags
valid-alignments: data_all/dev/dev.src-mt.alignments

# vocabulary
source-vocab-min-frequency: 2
target-vocab-min-frequency: 2
keep-rare-words-with-embeddings: true
add-embeddings-vocab: false
